"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[130],{7735:n=>{n.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025/11/19/welcome/BN_LN","metadata":{"permalink":"/blog/2025/11/19/welcome/BN_LN","editUrl":"https://github.com/zhangyan-didu/zhangyan-didu.github.io/tree/main/blog/2025-11-19-welcome/BN_LN.md","source":"@site/blog/2025-11-19-welcome/BN_LN.md","title":"\u7ec6\u7a76Batch Norm\u4e0eLayer Norm","description":"\u95ee\u9898\u80cc\u666f\uff1a","date":"2025-11-19T00:00:00.000Z","tags":[],"readingTime":7.75,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"nextItem":{"title":"DPO\u7b97\u6cd5\u8be6\u89e3","permalink":"/blog/2025/11/19/welcome/DPO"}},"content":"### \u95ee\u9898\u80cc\u666f\uff1a\\n\\n\u7ecf\u5e38\u7528\u5230Batch Norm\u548cLayer Norm\u64cd\u4f5c\uff0c\u4f46\u662f\u5bf9\u4e24\u8005\u7684\u7406\u89e3\u4e5f\u53ea\u505c\u7559\u5728\u65b9\u4fbf\u8bad\u7ec3\u3001LN\u9002\u7528\u4e8eNLP\u3001BN\u9002\u7528\u4e8eCV\u7684\u7a0b\u5ea6\u3002\u4e3a\u4e86\u66f4\u597d\u7406\u89e3\u4e24\u4e2a\u64cd\u4f5c\u7684\u7406\u8bba\u610f\u4e49\uff0c\u8fdb\u884c\u4e86\u4e00\u4e9b\u5b66\u4e60\u3002\u4e0b\u9762\u4e88\u4ee5\u5c55\u5f00\u3002\\n\\n#### Normalization\u516c\u5f0f\\n\\n$y=\\\\frac{x-E[x]}{\\\\sqrt{Var[x]+\\\\epsilon}}*\\\\alpha + \\\\beta$\\n\\n\u5206\u6bcd\u4e2d\u7684 $\\\\epsilon$ \u4f5c\u7528\u662f\u9632\u6b62\u5206\u6bcd\u4e3a0\u3002\\n\\n$\\\\frac{x-E[x]}{\\\\sqrt{Var[x]+\\\\epsilon}}$\u64cd\u4f5c\u540e\u8fd8\u8981\u8fdb\u884c\u4eff\u5c04\u53d8\u6362($*\\\\alpha, +\\\\beta$)\u3002\u539f\u56e0\u662f\uff1a\\n\\nNormalization\u901a\u5e38\u5728\u6fc0\u6d3b\u51fd\u6570\u7684\u524d\u4e00\u6b65\u8fdb\u884c\uff0c\u5982\u679c\u4e0d\u8fdb\u884c\u4eff\u5c04\u53d8\u6362\uff0c\u5219\u8f93\u51fa\u7684\u6570\u636e\u5747\u503c\u4e3a0\uff0c\u65b9\u5dee\u4e3a1\uff0c\u5728\u8fd9\u4e2a\u53d8\u5316\u8303\u56f4\u5185\u7684\u6fc0\u6d3b\u51fd\u6570\u66f2\u7ebf\u57fa\u672c\u662f\u7ebf\u6027\u7684\uff0c\u62b5\u6d88\u4e86\u6fc0\u6d3b\u51fd\u6570\u5e26\u6765\u7684\u975e\u7ebf\u6027\u4ef7\u503c\u3002\\n\\n### pytorch\u4ee3\u7801\u5c55\u793a\\n\\n```python\\nclass BatchNorm2d(_BatchNorm):\\n    r\\"\\"\\"Applies Batch Normalization over a 4D input.\\n\\n    4D is a mini-batch of 2D inputs\\n    with additional channel dimension. Method described in the paper\\n    `Batch Normalization: Accelerating Deep Network Training by Reducing\\n    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\\n\\n    .. math::\\n\\n        y = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\beta\\n\\n    The mean and standard-deviation are calculated per-dimension over\\n    the mini-batches and :math:`\\\\gamma` and :math:`\\\\beta` are learnable parameter vectors\\n    of size `C` (where `C` is the input size). By default, the elements of :math:`\\\\gamma` are set\\n    to 1 and the elements of :math:`\\\\beta` are set to 0. At train time in the forward pass, the\\n    standard-deviation is calculated via the biased estimator, equivalent to\\n    ``torch.var(input, unbiased=False)``. However, the value stored in the moving average of the\\n    standard-deviation is calculated via the unbiased  estimator, equivalent to\\n    ``torch.var(input, unbiased=True)``.\\n\\n    Also by default, during training this layer keeps running estimates of its\\n    computed mean and variance, which are then used for normalization during\\n    evaluation. The running estimates are kept with a default :attr:`momentum`\\n    of 0.1.\\n\\n    If :attr:`track_running_stats` is set to ``False``, this layer then does not\\n    keep running estimates, and batch statistics are instead used during\\n    evaluation time as well.\\n\\n    .. note::\\n        This :attr:`momentum` argument is different from one used in optimizer\\n        classes and the conventional notion of momentum. Mathematically, the\\n        update rule for running statistics here is\\n        :math:`\\\\hat{x}_\\\\text{new} = (1 - \\\\text{momentum}) \\\\times \\\\hat{x} + \\\\text{momentum} \\\\times x_t`,\\n        where :math:`\\\\hat{x}` is the estimated statistic and :math:`x_t` is the\\n        new observed value.\\n\\n    Because the Batch Normalization is done over the `C` dimension, computing statistics\\n    on `(N, H, W)` slices, it\'s common terminology to call this Spatial Batch Normalization.\\n\\n    Args:\\n        num_features: :math:`C` from an expected input of size\\n            :math:`(N, C, H, W)`\\n        eps: a value added to the denominator for numerical stability.\\n            Default: 1e-5\\n        momentum: the value used for the running_mean and running_var\\n            computation. Can be set to ``None`` for cumulative moving average\\n            (i.e. simple average). Default: 0.1\\n        affine: a boolean value that when set to ``True``, this module has\\n            learnable affine parameters. Default: ``True``\\n        track_running_stats: a boolean value that when set to ``True``, this\\n            module tracks the running mean and variance, and when set to ``False``,\\n            this module does not track such statistics, and initializes statistics\\n            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\\n            When these buffers are ``None``, this module always uses batch statistics.\\n            in both training and eval modes. Default: ``True``\\n\\n    Shape:\\n        - Input: :math:`(N, C, H, W)`\\n        - Output: :math:`(N, C, H, W)` (same shape as input)\\n\\n    Examples::\\n\\n        >>> # With Learnable Parameters\\n        >>> m = nn.BatchNorm2d(100)\\n        >>> # Without Learnable Parameters\\n        >>> m = nn.BatchNorm2d(100, affine=False)\\n        >>> input = torch.randn(20, 100, 35, 45)\\n        >>> output = m(input)\\n    \\"\\"\\"\\n\\n    def _check_input_dim(self, input):\\n        if input.dim() != 4:\\n            raise ValueError(f\\"expected 4D input (got {input.dim()}D input)\\")\\n\\nclass BatchNorm2d(\\n    num_features: int,\\n    eps: float = 0.00001,\\n    momentum: float | None = 0.1,\\n    affine: bool = True,\\n    track_running_stats: bool = True,\\n    device: Any | None = None,\\n    dtype: Any | None = None\\n)\\n```\\n\\n`batch_norm`\u56fa\u5b9a\u5bf9\u6570\u636e\u7684\u7279\u5f81\u7ef4\u5ea6\u8fdb\u884c\u5f52\u4e00\u5316\u3002\\n\\n\u4e3e\u4f8b\uff1a\\n\\n\u8f93\u5165\u6570\u636eshape (2, 3, 4, 4)\uff0c\uff082\u6761\u6570\u636e\uff0c3\u4e2a\u7279\u5f81\u7ef4\u5ea6\uff0c4*4\u5927\u5c0f\uff09\u5bf9\u5176\u8fdb\u884cBatch Norm\uff0c\u5c31\u662f\u5728\u7279\u5f81\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u7684\u3002\u5747\u503c\u7684shape\u4e3a(1, 3, 1, 1)\\n\\n\\n\\n```python\\nclass LayerNorm(Module):\\n    r\\"\\"\\"Applies Layer Normalization over a mini-batch of inputs.\\n\\n    This layer implements the operation as described in\\n    the paper `Layer Normalization <https://arxiv.org/abs/1607.06450>`__\\n\\n    .. math::\\n        y = \\\\frac{x - \\\\mathrm{E}[x]}{ \\\\sqrt{\\\\mathrm{Var}[x] + \\\\epsilon}} * \\\\gamma + \\\\beta\\n\\n    The mean and standard-deviation are calculated over the last `D` dimensions, where `D`\\n    is the dimension of :attr:`normalized_shape`. For example, if :attr:`normalized_shape`\\n    is ``(3, 5)`` (a 2-dimensional shape), the mean and standard-deviation are computed over\\n    the last 2 dimensions of the input (i.e. ``input.mean((-2, -1))``).\\n    :math:`\\\\gamma` and :math:`\\\\beta` are learnable affine transform parameters of\\n    :attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``.\\n    The variance is calculated via the biased estimator, equivalent to\\n    `torch.var(input, unbiased=False)`.\\n\\n    .. note::\\n        Unlike Batch Normalization and Instance Normalization, which applies\\n        scalar scale and bias for each entire channel/plane with the\\n        :attr:`affine` option, Layer Normalization applies per-element scale and\\n        bias with :attr:`elementwise_affine`.\\n\\n    This layer uses statistics computed from input data in both training and\\n    evaluation modes.\\n\\n    Args:\\n        normalized_shape (int or list or torch.Size): input shape from an expected input\\n            of size\\n\\n            .. math::\\n                [* \\\\times \\\\text{normalized\\\\_shape}[0] \\\\times \\\\text{normalized\\\\_shape}[1]\\n                    \\\\times \\\\ldots \\\\times \\\\text{normalized\\\\_shape}[-1]]\\n\\n            If a single integer is used, it is treated as a singleton list, and this module will\\n            normalize over the last dimension which is expected to be of that specific size.\\n        eps: a value added to the denominator for numerical stability. Default: 1e-5\\n        elementwise_affine: a boolean value that when set to ``True``, this module\\n            has learnable per-element affine parameters initialized to ones (for weights)\\n            and zeros (for biases). Default: ``True``.\\n        bias: If set to ``False``, the layer will not learn an additive bias (only relevant if\\n            :attr:`elementwise_affine` is ``True``). Default: ``True``.\\n\\n    Attributes:\\n        weight: the learnable weights of the module of shape\\n            :math:`\\\\text{normalized\\\\_shape}` when :attr:`elementwise_affine` is set to ``True``.\\n            The values are initialized to 1.\\n        bias:   the learnable bias of the module of shape\\n                :math:`\\\\text{normalized\\\\_shape}` when :attr:`elementwise_affine` is set to ``True``.\\n                The values are initialized to 0.\\n\\n    Shape:\\n        - Input: :math:`(N, *)`\\n        - Output: :math:`(N, *)` (same shape as input)\\n\\n    Examples::\\n\\n        >>> # NLP Example\\n        >>> batch, sentence_length, embedding_dim = 20, 5, 10\\n        >>> embedding = torch.randn(batch, sentence_length, embedding_dim)\\n        >>> layer_norm = nn.LayerNorm(embedding_dim)\\n        >>> # Activate module\\n        >>> layer_norm(embedding)\\n        >>>\\n        >>> # Image Example\\n        >>> N, C, H, W = 20, 5, 10, 10\\n        >>> input = torch.randn(N, C, H, W)\\n        >>> # Normalize over the last three dimensions (i.e. the channel and spatial dimensions)\\n        >>> # as shown in the image below\\n        >>> layer_norm = nn.LayerNorm([C, H, W])\\n        >>> output = layer_norm(input)\\n\\n    .. image:: ../_static/img/nn/layer_norm.jpg\\n        :scale: 50 %\\n\\n    \\"\\"\\"\\n\\n    __constants__ = [\\"normalized_shape\\", \\"eps\\", \\"elementwise_affine\\"]\\n    normalized_shape: tuple[int, ...]\\n    eps: float\\n    elementwise_affine: bool\\n\\n    def __init__(\\n        self,\\n        normalized_shape: _shape_t,\\n        eps: float = 1e-5,\\n        elementwise_affine: bool = True,\\n        bias: bool = True,\\n        device=None,\\n        dtype=None,\\n    ) -> None:\\n        factory_kwargs = {\\"device\\": device, \\"dtype\\": dtype}\\n        super().__init__()\\n        if isinstance(normalized_shape, numbers.Integral):\\n            # mypy error: incompatible types in assignment\\n            normalized_shape = (normalized_shape,)  # type: ignore[assignment]\\n        self.normalized_shape = tuple(normalized_shape)  # type: ignore[arg-type]\\n        self.eps = eps\\n        self.elementwise_affine = elementwise_affine\\n        if self.elementwise_affine:\\n            self.weight = Parameter(\\n                torch.empty(self.normalized_shape, **factory_kwargs)\\n            )\\n            if bias:\\n                self.bias = Parameter(\\n                    torch.empty(self.normalized_shape, **factory_kwargs)\\n                )\\n            else:\\n                self.register_parameter(\\"bias\\", None)\\n        else:\\n            self.register_parameter(\\"weight\\", None)\\n            self.register_parameter(\\"bias\\", None)\\n\\n        self.reset_parameters()\\n\\n    def reset_parameters(self) -> None:\\n        if self.elementwise_affine:\\n            init.ones_(self.weight)\\n            if self.bias is not None:\\n                init.zeros_(self.bias)\\n\\n    def forward(self, input: Tensor) -> Tensor:\\n        return F.layer_norm(\\n            input, self.normalized_shape, self.weight, self.bias, self.eps\\n        )\\n\\n    def extra_repr(self) -> str:\\n        return (\\n            \\"{normalized_shape}, eps={eps}, \\"\\n            \\"elementwise_affine={elementwise_affine}\\".format(**self.__dict__)\\n        )\\n\\nclass LayerNorm(\\n    normalized_shape: _shape_t,\\n    eps: float = 0.00001,\\n    elementwise_affine: bool = True,\\n    bias: bool = True,\\n    device: Any | None = None,\\n    dtype: Any | None = None\\n)\\n```\\n\\n`layer_norm`\u63d0\u4f9b\u7684`normalized_shape`\u7528\u4ee5\u6307\u5b9a\u672b\u5c3e\u82e5\u5e72\u7ef4\u5ea6\uff0c\u4e00\u822c\u800c\u8a00\u5bf9\u4e00\u6761\u6570\u636e\u4e2d\u7684\u4e00\u4e2atoken\u7684\u6240\u6709\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\u3002\\n\\n\u4e3e\u4f8b\uff1a\\n\\n\u8f93\u5165\u6570\u636eshape (2, 5, 5)\uff0c\uff082\u6761\u6570\u636e\uff0c\u6bcf\u67615\u4e2atoken\uff0c\u7279\u5f81\u7ef4\u5ea6\u4e3a5\uff09\u5bf9\u5176\u5728\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u4e0a\u8fdb\u884cLayer Norm\uff0c\u5219\u5747\u503c\u7684shape\u4e3a(2, 5, 1)\\n\\n### \u4e3a\u4ec0\u4e48BN\u9002\u5408CV\uff0cLN\u9002\u5408NLP\\n\\n**BN**\u5728 **batch** \u7ef4\u5ea6\u4e0a\u8ba1\u7b97\u7edf\u8ba1\u91cf\uff0c\u5bf9\u4e8e\u4e0d\u540c\u6837\u672c\u7684\u540c\u4e00\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff0c**LN**\u5728**\u7279\u5f81**\u7ef4\u5ea6\u4e0a\u8ba1\u7b97\u7edf\u8ba1\u91cf\uff0c\u5bf9\u4e8e\u540c\u4e00\u6837\u672c\u5185\u7684\u4e0d\u540c\u7279\u5f81\u503c\u8fdb\u884c\u5f52\u4e00\u5316\u3002\\n\\n\u6709\u4e86\u4ee5\u4e0a\u7684\u57fa\u7840\u4fe1\u606f\uff0c\u518d\u6765\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\u3002\\n\\n\u5bf9\u4e8e\u56fe\u50cf\u6570\u636e\uff0c\u5176\u7a7a\u95f4\u4e0d\u53d8\u6027\uff08\u540c\u4e00\u7269\u4f53\u5728\u4e0d\u540c\u4f4d\u7f6e\u5177\u6709\u76f8\u4f3c\u7684\u7279\u5f81\u54cd\u5e94\uff09\u4f7f\u5f97\u5176\u5e0c\u671b**\u4e0d\u540c\u56fe\u50cf\u4e2d\u76f8\u540c\u8bed\u4e49\u7684\u7279\u5f81\u5728\u6570\u503c\u5206\u5e03\u4e0a\u4fdd\u6301\u4e00\u81f4**\u3002\u4e14\u56fe\u50cf\u6570\u636e\u7684\u683c\u5f0f\u5c5e\u6027\uff08\u56fe\u7247\u5927\u5c0f\u65b9\u4fbf\u5bf9\u9f50\uff09\u4f7f\u5f97BN\u64cd\u4f5c\u5bb9\u6613\u8fdb\u884c\u3002\\n\\n\u5bf9\u4e8e\u6587\u672c\u6570\u636e\uff0c\u5176\u4f4d\u7f6e\u654f\u611f\u6027\uff08**\u76f8\u540c\u5b57\u6bb5\u5728\u4e0d\u540c\u53e5\u5b50\u4e2d\u7684\u4e0d\u540c\u4f4d\u7f6e\u53ef\u80fd\u8868\u8fbe\u4e0d\u540c\u7684\u8bed\u4e49**\uff09\u5bfc\u81f4BN\u64cd\u4f5c\u4e0d\u9002\u5408\u6587\u672c\u6570\u636e\uff0c\u5f3a\u884c\u5f15\u5165BN\u53ef\u80fd\u5bfc\u81f4\u6587\u672c\u7684\u4f4d\u7f6e\u4fe1\u606f\u5931\u6548\uff0c\u4e14\u6587\u672c\u7684\u683c\u5f0f\u5c5e\u6027\uff08\u4e00\u822c\u90fd\u662f\u53d8\u957f\u6570\u636e\uff09\u4e5f\u4e0d\u65b9\u4fbf\u8fdb\u884cBN\u64cd\u4f5c\u3002\u800cLN\u65e2\u4fdd\u6301\u4e86\u5f52\u4e00\u5316\u64cd\u4f5c\u5e26\u6765\u7684\u6570\u503c\u7a33\u5b9a\u3001\u5229\u4e8e\u8bad\u7ec3\u7684benefit\uff0c\u53c8\u4fdd\u6301\u4e86\u53e5\u5b50\u5185\u90e8\u7279\u5f81\u5206\u5e03\u7684\u72ec\u7acb\u6027\uff0c\u4e14\u9002\u5408\u53d8\u957f\u6570\u636e\u3002"},{"id":"/2025/11/19/welcome/DPO","metadata":{"permalink":"/blog/2025/11/19/welcome/DPO","editUrl":"https://github.com/zhangyan-didu/zhangyan-didu.github.io/tree/main/blog/2025-11-19-welcome/DPO.md","source":"@site/blog/2025-11-19-welcome/DPO.md","title":"DPO\u7b97\u6cd5\u8be6\u89e3","description":"Background\uff1a","date":"2025-11-19T00:00:00.000Z","tags":[],"readingTime":6.18,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"\u7ec6\u7a76Batch Norm\u4e0eLayer Norm","permalink":"/blog/2025/11/19/welcome/BN_LN"},"nextItem":{"title":"GEMM\u4f18\u5316\u7b14\u8bb0","permalink":"/blog/gemm-optimization"}},"content":"#### Background\uff1a\\n\\n**RLHF(Reinforcement Learning from Human Feedback)\uff1a**\\n\\n\u6d41\u7a0b\uff1a\\n\\n\u9996\u5148\u7531\u4e00\u7ec4\u5177\u6709\u4eba\u7c7b\u504f\u597d\u7684\u6570\u636e\uff0c\u7136\u540e\u4f7f\u7528\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u65b9\u6cd5\u5f97\u5230\u4e00\u4e2a **reward model**\uff0creward model\u53ef\u4ee5\u5728\u7ed9\u5b9a prompt \u548c\u5927\u8bed\u8a00\u6a21\u578b\u56de\u7b54\u7684\u60c5\u51b5\u4e0b\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56de\u7b54\u8fdb\u884c\u6253\u5206\u3002\u5229\u7528reward model\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002\\n\\n#### DPO\u7b97\u6cd5\u7b80\u4ecb\uff1a\\n\\n\u76f4\u63a5\u4ece\u7528\u6237\u504f\u597d\u6570\u636e\u4e2d\u8fdb\u884c\u5b66\u4e60\uff0c\u901a\u8fc7\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u83b7\u5f97\u6700\u7ec8\u6a21\u578b\u3002\\n\\n##### Benefit\uff1a\\n\\n\u6d41\u7a0b\u6781\u5927\u7b80\u5316\\n\\n#### \u524d\u7f6e\u77e5\u8bc6\uff1a\\n\\n**KL\u6563\u5ea6\uff1a**\\n\\n\u5b9a\u4e49\uff1a\\n\\nP \u5206\u5e03\u76f8\u5bf9\u4e8e Q \u5206\u5e03\u7684\u76f8\u4f3c\u7a0b\u5ea6\u3002KL\u6563\u5ea6\u7684\u503c\u5927\u4e8e\u7b49\u4e8e0\u3002P\u548cQ\u8d8a\u76f8\u4f3c\uff0cKL\u6563\u5ea6\u8d8a\u63a5\u8fd10 \u3002\u5982\u679cP\u548cQ\u5206\u5e03\u5b8c\u5168\u4e00\u81f4\uff0c\u5219KL\u6563\u5ea6\u7b49\u4e8e0\u3002\\n\\n\u516c\u5f0f\uff1a\\n$$\\nKL(P||Q) = \\\\sum_{x \\\\in X}P(x)log(\\\\frac{P(x)}{Q(x)})\\n$$\\n\u6ce8\u91ca\uff1a\\n\\nP\u76f8\u5bf9\u4e8eQ\u7684KL\u6563\u5ea6\uff0c\u548c Q\u76f8\u5bf9\u4e8eP\u7684KL\u6563\u5ea6\uff0c\u662f\u4e0d\u4e00\u6837\u7684\u3002\\n\\n\u5173\u4e8e\u4e3a\u4f55KL\u6563\u5ea6\u5927\u4e8e\u7b49\u4e8e0\uff0c\u76f4\u89c2\u7406\u89e3\uff1a\u5982\u679clog\u90e8\u5206\u662f\u5c0f\u4e8e0\u7684\uff0c\u5219\u5206\u5b50\u4e00\u5b9a\u6bd4\u8f83\u5c0f\uff0c\u90a3\u4e48P(x)\u76f8\u5bf9\u5c31\u504f\u5c0f\uff0c\u800c\u5982\u679clog\u90e8\u5206\u5927\u4e8e0\uff0c\u5219\u5206\u5b50\u504f\u5927\uff0c\u90a3\u4e48P(x)\u5c31\u504f\u5927\u3002\u8fd9\u6837\u4e58\u4ee5\u7cfb\u6570\u4e4b\u540e\u4e00\u5b9a\u53ef\u4ee5\u5f97\u5230\u5927\u4e8e0\u7684\u503c\u3002\\n\\n**Bradley-Terry\u6a21\u578b\uff1a**\\n\\n\u529f\u80fd\uff1a\\n\\n\u5bf9\u6bd4\u8f83\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u3002\u4f8b\u5982\uff0c\u5df2\u77e5A\u4e0eB\u7684\u5bf9\u6218\u7ed3\u679c\uff0cA\u4e0eC\u7684\u5bf9\u6218\u7ed3\u679c\uff0c\u6c42B\u4e0eC\u7684\u5bf9\u6218\u9884\u671f\u7ed3\u679c\u3002\\n\\n\u5b9a\u4e49\uff1a\\n$$\\nP(i>j) = \\\\frac{\\\\alpha_i}{\\\\alpha_i + \\\\alpha_j}\\n$$\\n\u5047\u8bbe\u6bcf\u4e2a\u51fd\u6570\u90fd\u6709\u4e00\u4e2a\u9690\u542b\u7684\u5b9e\u529b\u53c2\u6570\uff0c$\\\\alpha_i$\u8868\u793a\u7b2c i \u4e2a\u5143\u7d20\u7684\u5b9e\u529b\uff0c$P(i>j)$\u8868\u793a\u7b2c i \u4e2a\u5143\u7d20\u6218\u80dc\u7b2c j \u4e2a\u5143\u7d20\u7684\u6982\u7387\u3002\\n\\n\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5bf9\u5df2\u77e5\u6570\u636e\u505a**\u5bf9\u6570\u6700\u5927\u4f3c\u7136\u4f30\u8ba1**\u5f97\u5230\u5b9e\u529b\u53c2\u6570\u7684\u503c\u3002\\n\\n\u636e\u6b64\u5b9a\u4e49\u4e00\u4e2a\u4e00\u822c\u7684Loss\u51fd\u6570\uff0cLoss\u8d8a\u5c0f\u8d8a\u597d\uff1a\\n$$\\nLoss=-\\\\mathbb{E}_{(\\\\alpha_x, \\\\alpha_y)~D}[ln\\\\frac{\\\\alpha_x}{\\\\alpha_x+\\\\alpha_y}]\\n$$\\n\u8fd9\u5b9e\u9645\u5c31\u662f\u4e00\u4e2a\u5206\u7c7b\u95ee\u9898\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u3002\\n\\n\u6211\u4eec\u7684\u4f18\u5316\u76ee\u6807\u5c31\u662f\u8ba9x\uff08\u597d\u56de\u7b54\uff09\u6218\u80dcy\uff08\u574f\u56de\u7b54\uff09\u7684\u6982\u7387\u8d8b\u8fd1\u4e8e1\u3002\\n\\n**\u5f3a\u5316\u5b66\u4e60\u91cc\uff1a**\\n\\n\u8f93\u5165\u7684prompt\u662fx\uff0c\u56de\u7b54\u662fy\u3002\u56de\u7b54y\u7684\u597d\u574f\u901a\u8fc7reward\u6a21\u578b\u6765\u8bc4\u4f30\u3002\\n$$\\nP(y_1>y_2)=\\\\frac{r(x,y_1)}{r(x,y_1)+r(x,y_2)}\\n$$\\nr(x,y)\u6709\u53ef\u80fd\u8fd4\u56de\u8d1f\u6570\uff0c\u6240\u4ee5\u52a0\u4e0a\u6307\u6570\u51fd\u6570\\n$$\\nP(y_1>y_2)=\\\\frac{exp(r(x,y_1))}{exp(r(x,y_1)+r(x,y_2))}\\n$$\\n**Bradley_Terry\u6a21\u578b**\\n$$\\nP(y_w>y_l)=\\\\frac{exp(r(x,y_w))}{exp(r(x,y_w))+exp(r(x,y_l))}\\n$$\\n\\n$$\\n\\\\sigma(x)=\\\\frac{1}{1+exp(-x)}\\n$$\\n\\n$$\\n\\\\begin{aligned}\\nLoss &= -\\\\mathbb{E}_{(x,y_w,y_l)\\\\sim D}\\\\left[ln\\\\frac{\\\\exp(r(x,y_w))}{\\\\exp(r(x,y_w))+\\\\exp(r(x,y_l))}\\\\right] \\\\\\\\\\n&= -\\\\mathbb{E}_{(x,y_w,y_l)\\\\sim D}[ln\\\\frac{1}{1+exp(r(x,y_l)-r(x,y_w))}]\\\\\\\\\\n&=-\\\\mathbb{E}_{(x,y_w,y_l)\\\\sim D}[ln\\\\sigma(r(x,y_w)-r(x,y_l))]\\n\\\\end{aligned}\\n$$\\n\\n$$\\n-ln\\\\sigma(r(x,y_w)-r(x,y_l))\\n$$\\n\\n\u8ba9Loss\u5c3d\u53ef\u80fd\u7684\u5c0f\uff0c\u4e5f\u5c31\u662f\u8ba9$y_w$\u901a\u8fc7reward\u65b9\u6cd5\u7684\u5f97\u5206\u5c3d\u53ef\u80fd\u591a\u4e8e$y_l$\u7684\u5f97\u5206\\n\\n#### DPO\u7684\u8bad\u7ec3\u76ee\u6807\\n\\n\u5956\u52b1\u51fd\u6570\uff1a\\n\\n$r(x,y)\\\\ \\\\ x:prompt\\\\ \\\\ y:response$\\n\\n\u57fa\u51c6\u6a21\u578b\uff1a\\n\\n$\\\\pi_{ref}(y|x)$\\n\\n\u8bad\u7ec3\u6a21\u578b\uff1a\\n\\n$\\\\pi(y|x)$\\n\\n**\u8bad\u7ec3\u76ee\u6807\uff1a**\\n$$\\n\\\\underset{\\\\pi}{max}\\\\mathbb{E}_{x\\\\sim D,y\\\\sim\\\\pi}[r(x,y)]-\\\\beta\\\\mathbb{D}_{KL}[\\\\pi(y|x)||\\\\pi_{ref}(y|x)]\\n$$\\n\u5de6\u534a\u90e8\u5206\uff1a\u5f97\u5230\u5c3d\u53ef\u80fd\u591a\u7684\u5956\u52b1\\n\\n\u53f3\u534a\u90e8\u5206\uff1a\u65b0\u8bad\u7ec3\u7684\u6a21\u578b\u5c3d\u53ef\u80fd\u548c\u57fa\u51c6\u6a21\u578b\u5206\u5e03\u4e00\u81f4\u3002\u5176\u4e2d$\\\\beta$\u662f\u8d85\u53c2\u6570\uff0c$\\\\beta$\u8d8a\u5927\uff0c\u8868\u793a\u5206\u5e03\u8d8a\u5e94\u8be5\u4e00\u81f4\\n$$\\n\\\\begin{aligned}\\n&\\\\quad\\\\ \\\\underset{\\\\pi}{max}\\\\mathbb{E}_{x\\\\sim D,y\\\\sim\\\\pi}[r(x,y)]-\\\\beta\\\\mathbb{D}_{KL}[\\\\pi(y|x)||\\\\pi_{ref}(y|x)]\\\\\\\\\\n&=\\\\underset{\\\\pi}{max}\\\\mathbb{E}_{x\\\\sim D,y\\\\sim\\\\pi}[r(x,y)]-\\\\mathbb{E}_{x\\\\sim D,y\\\\sim\\\\pi}[\\\\beta log\\\\frac{\\\\pi(y|x)}{\\\\pi_{ref}(y|x)}]\\\\\\\\\\n&=\\\\underset{\\\\pi}{max}\\\\mathbb{E}_{x\\\\sim D,y\\\\sim\\\\pi}[r(x,y)-\\\\beta log\\\\frac{\\\\pi(y|x)}{\\\\pi_{ref}(y|x)}]\\\\\\\\\\n&=\\\\underset{\\\\pi}{min}\\\\mathbb{E}_{x\\\\sim D,y\\\\sim\\\\pi}[log\\\\frac{\\\\pi(y|x)}{\\\\pi_{ref}(y|x)}-\\\\frac{1}{\\\\beta}r(x,y)]\\n\\\\end{aligned}\\n$$\\n\uff08\u53d6$min$\u4e4b\u540e\uff0c\u4e24\u9879\u6362\u4f4d\u7f6e\u4e14\u4e24\u9879\u5747\u9664\u4ee5$\\\\beta$\uff09\\n$$\\n\\\\begin{aligned}\\n&=\\\\underset{\\\\pi}{min}\\\\mathbb{E}_{x\\\\sim D,y\\\\sim\\\\pi}[log\\\\frac{\\\\pi(y|x)}{\\\\pi_{ref}(y|x)}-\\\\frac{1}{\\\\beta}r(x,y)]\\\\\\\\\\n&=\\\\underset{\\\\pi}{min}\\\\mathbb{E}_{x\\\\sim D,y\\\\sim\\\\pi}[log\\\\frac{\\\\pi(y|x)}{\\\\pi_{ref}(y|x)}-log\\\\ exp\\\\frac{1}{\\\\beta}r(x,y)]\\\\\\\\\\n&=\\\\underset{\\\\pi}{min}\\\\mathbb{E}_{x\\\\sim D,y\\\\sim\\\\pi}[log\\\\frac{\\\\pi(y|x)}{\\\\pi_{ref}(y|x)exp\\\\frac{1}{\\\\beta}r(x,y)}]\\\\\\\\\\n&=\\\\underset{\\\\pi}{min}\\\\mathbb{E}_{x\\\\sim D,y\\\\sim\\\\pi}[log\\\\frac{\\\\pi(y|x)}{\\\\pi_{ref}(y|x)exp\\\\frac{1}{\\\\beta}r(x,y)\\\\frac{1}{Z(x)}Z(x)}]\\\\\\\\\\n&=\\\\underset{\\\\pi}{min}\\\\mathbb{E}_{x\\\\sim D,y\\\\sim\\\\pi}[log\\\\frac{\\\\pi(y|x)}{\\\\frac{1}{Z(x)}\\\\pi_{ref}(y|x)exp\\\\frac{1}{\\\\beta}r(x,y)}-logZ(x)]\\\\\\\\\\n\\\\end{aligned}\\n$$\\n\\n$$\\nZ(x)=\\\\sum_{y}\\\\pi_{ref}(y|x)exp(\\\\frac{1}{\\\\beta}r(x,y))\\n$$\\n\\n$$\\n{\\\\frac{1}{Z(x)}\\\\pi_{ref}(y|x)exp\\\\frac{1}{\\\\beta}r(x,y)}=\\\\frac{\\\\pi_{ref}(y|x)exp(\\\\frac{1}{\\\\beta}r(x,y))}{\\\\sum_{y}\\\\pi_{ref}(y|x)exp(\\\\frac{1}{\\\\beta}r(x,y))}=\\\\pi^*(y|x)\\n$$\\n\\n\u4e5f\u5c31\u662f\u5bf9\u4e8e\u4e00\u4e2ax\uff0c\u300c\u4e00\u79cd\u7279\u5b9a\u7684y\u7684\u6982\u7387\u300d\u6bd4\u4e0a\u300c\u6240\u6709y\u7684\u6982\u7387\u4e4b\u548c\u300d\\n$$\\n\\\\begin{aligned}\\n&=\\\\underset{\\\\pi}{min}\\n\\\\mathbb{E}_{x\\\\sim D,y\\\\sim\\\\pi}[log\\\\frac{\\\\pi(y|x)}{\\\\pi^*(y|x)}-logZ(x)]\\\\\\\\\\n&=\\\\underset{\\\\pi}{min}\\\\mathbb{E}_{x\\\\sim D,y\\\\sim\\\\pi}[log\\\\frac{\\\\pi(y|x)}{\\\\pi^*(y|x)}]\\\\\\\\\\n&=\\\\underset{\\\\pi}{min}\\\\mathbb{E}_{x\\\\sim D}[\\\\mathbb{D}_{KL}(\\\\pi(y|x)||\\\\pi^*(y|x))]=>\\\\pi(y|x)=\\\\pi^*(y|x)=\\\\frac{1}{Z(x)}\\\\pi_{ref}(y|x)exp(\\\\frac{1}{\\\\beta}r(x,y))\\n\\n\\\\end{aligned}\\n$$\\n\u6700\u540e\u4e00\u6b65\uff1a\u8981\u8ba9KL\u6563\u5ea6\u503c\u6700\u5c0f\uff0c\u4e5f\u5c31\u662f\u4e24\u4e2a\u5206\u5e03\u4e00\u6837\uff0c\u5373\u5f97\u4ee5\u4e0a\u5185\u5bb9\\n$$\\n\\\\begin{aligned}\\n&\\\\pi(y|x)=\\\\frac{1}{Z(x)}\\\\pi_{ref}(y|x)exp(\\\\frac{1}{\\\\beta}r(x,y))\\\\\\\\\\n&=>exp(\\\\frac{1}{\\\\beta}r(x,y))=\\\\frac{\\\\pi(y|x)}{\\\\pi_{ref}(y|x)}Z(x)\\\\\\\\\\n&=>r(x,y)=\\\\beta ln(\\\\frac{\\\\pi(y|x)}{\\\\pi_{ref}(y|x)}Z(x))\\\\\\\\\\n&=>r(x,y)=\\\\beta ln(\\\\frac{\\\\pi(y|x)}{\\\\pi_{ref}(y|x)}Z(x))+\\\\beta lnZ(x)\\n\\\\end{aligned}\\n$$\\n\u5f97\u5230**reward function**\u7684\u4e00\u4e2a\u8868\u793a\\n\\n\u53c8\u77e5\u9053 **Bradley Terry**\u4e2d\u7684Loss\u51fd\u6570\u4e3a\uff1a\\n$$\\n-ln\\\\sigma(r(x,y_w)-r(x,y_l))\\n$$\\n\u4ee3\u5165r\u7684\u8868\u8fbe\u5f0f\uff0c\u53ef\u5f97\uff1a\\n$$\\n-\\\\ln\\\\sigma(\\\\beta ln\\\\frac{\\\\pi(y_w|x)}{\\\\pi_{ref}(y_w|x)}Z(x)+\\\\beta lnZ(x)-\\\\beta ln\\\\frac{\\\\pi(y_l|x)}{\\\\pi_{ref}(y_l|x)}-\\\\beta lnZ(x))\\n$$\\n**DPO Loss:**\\n$$\\n-\\\\ln\\\\sigma(\\\\beta ln\\\\frac{\\\\pi(y_w|x)}{\\\\pi_{ref}(y_w|x)}Z(x)-\\\\beta ln\\\\frac{\\\\pi(y_l|x)}{\\\\pi_{ref}(y_l|x)})\\n$$\\n$\\\\sigma(x)=\\\\frac{1}{1+exp(-x)}$"},{"id":"gemm-optimization","metadata":{"permalink":"/blog/gemm-optimization","editUrl":"https://github.com/zhangyan-didu/zhangyan-didu.github.io/tree/main/blog/2025-11-19-welcome/index.md","source":"@site/blog/2025-11-19-welcome/index.md","title":"GEMM\u4f18\u5316\u7b14\u8bb0","description":"\u660e\u786e\u53c2\u6570\uff1a","date":"2025-11-01T00:00:00.000Z","tags":[{"inline":true,"label":"AI infra","permalink":"/blog/tags/ai-infra"},{"inline":true,"label":"SGEMM","permalink":"/blog/tags/sgemm"},{"inline":true,"label":"Basics","permalink":"/blog/tags/basics"}],"readingTime":8.57,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"gemm-optimization","title":"GEMM\u4f18\u5316\u7b14\u8bb0","tags":["AI infra","SGEMM","Basics"],"date":"2025-11-01T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"DPO\u7b97\u6cd5\u8be6\u89e3","permalink":"/blog/2025/11/19/welcome/DPO"}},"content":"### \u660e\u786e\u53c2\u6570\uff1a\\n\\nA, B, C\u7ef4\u5ea6\u5747\u4e3a2048\\\\*2048\u3002\\n\\n$b_m$ = $b_n$ = 128, $b_k$ = 8, $r_m$ = $r_n$ = 8\\n\\n\u5219\u9700\u8981\u5f00\u542f$\\\\frac{2048}{128}*\\\\frac{2048}{128} = 256$ \u4e2ablock\\n\\n \u6bcf\u4e2ablock\u5305\u542b$\\\\frac{128}{8}*\\\\frac{128}{8} = 256$\u4e2a\u7ebf\u7a0b\\n\\n\u6bcf\u4e2a\u7ebf\u7a0b\u8d1f\u8d23\u8ba1\u7b978*8=64\u4e2a\u5143\u7d20\u7684\u7ed3\u679c\\n\\n\u6bcf\u4e2ablock\u8d1f\u8d23256*64 = 16384\u4e2a\u5143\u7d20\u7684\u7ed3\u679c\\n\\n### \u4e0d\u91c7\u7528\u6570\u636e\u9884\u53d6\uff1a\\n\\n\u9488\u5bf9\u4e00\u4e2a\u5927\u5750\u6807\u4e3a(m,n)block (size=128\\\\*128)\u7684\u8ba1\u7b97\uff1a\\n\\n\u5b83\u9700\u8981A(size=2048\\\\*2048)\u7684\uff08m,\uff09,B(size=2048\\\\*2048)\u7684(,n)\uff0c\u4e00\u5171\u9700\u8981\u8fdb\u884c2048/8=256\u6b21\u8fed\u4ee3\uff08\u5927\u8fed\u4ee3\uff09\u3002\u6bcf\u6b21\u8fed\u4ee3\u9700\u8981\u53d6A\u4e2d\u7684128\\\\*8\u4e2a\u5143\u7d20\uff0cB\u4e2d\u76848\\\\*128\u4e2a\u5143\u7d20\u5230shared memory\u4e2d\u3002\u8fd9\u4e2ablock\u4e2d\u7684256\u4e2a\u7ebf\u7a0b\u628a\u7ed3\u679c\u8ba1\u7b97\u51fa\u6765\u3002\\n\\n\u9488\u5bf9\u4e00\u4e2ablock\u7684\u4e00\u6b21\u5927\u8fed\u4ee3\u7684\u8ba1\u7b97\uff1a\\n\\nshared memory\u4e2d\u6709128\\\\*8\u4e2aA\u77e9\u9635\u5143\u7d20\u548c8\\\\*128\u4e2aB\u77e9\u9635\u5143\u7d20\u3002\u6bcf\u4e2a\u7ebf\u7a0b\u8d1f\u8d23\u4e00\u4e2a8\\\\*8\u7684\u5c0f\u5757\uff0c\u6bcf\u4e2a\u7ebf\u7a0b\u9700\u8981\u8fdb\u884c8\u6b21\u8fed\u4ee3\uff08\u5c0f\u8fed\u4ee3\uff09\u3002A\u4e2d\u76848\\\\*8\u5750\u6807\u4e0b\u7684\u5927\u5750\u6807\u548cB\u4e2d\u76848\\\\*8\u5750\u6807\u4e0b\u7684\u5927\u5750\u6807\u76f8\u4e58\u5f97\u5230C\u4e2d\u7684\u5bf9\u5e94\u5750\u6807\u3002\u6bd4\u5982\u8bf4\uff0c\u9488\u5bf9\u4e8ek=8\u7684\u7279\u6b8a\u60c5\u51b5\uff0cA\u4e2d\u7684(0,0)\\\\*B\u4e2d\u7684(0,0)\u5757\u5f97\u5230C\u4e2d\u7684(0,0)\u5757\uff0cA\u4e2d\u7684(0,0)\\\\*B\u4e2d\u7684(0,1)\u5757\u5f97\u5230C\u4e2d\u7684(0,1)\u5757\\n\\n\u9488\u5bf9\u4e00\u4e2a8\\\\*8blcok\uff1a\\n\\n\u4f7f\u7528\u77e9\u9635\u5916\u79ef\u89c6\u89d2\uff0c\u6bcf\u6b21\u5c0f\u8fed\u4ee3\u53d6A\u77e9\u9635\u5f53\u524d\u5c0f\u5757\u7684\u4e00\u5217(8,1)\u548cB\u77e9\u9635\u5f53\u524d\u5c0f\u5757\u7684\u4e00\u884c(1,8)\uff0c\u7136\u540e\u4e0d\u65ad\u8fed\u4ee3\uff0c\u4e00\u5171\u8fed\u4ee38\\\\*8\u6b21\uff0c\u4e5f\u5c31\u662f64\u6761FFMA\u6307\u4ee4\uff0c\u6700\u7ec8\u5f97\u5230C\u4e2d\u5bf9\u5e94\u5c0f\u5757\u7684\u7ed3\u679c\u3002\\n\\n### \u91c7\u7528\u6570\u636e\u9884\u53d6\uff1a\\n\\n**\u4e3b\u8981\u533a\u522b\uff1a** \u5f00\u542f\u7684shared memory\u548c\u5bc4\u5b58\u5668\u6570\u91cf\u6709\u533a\u522b\uff08\u4e24\u500d\uff09\uff1b\u9700\u8981\u5c06\u4e00\u4e9b\u6570\u636e\u63d0\u524d\u653e\u7f6e\u5230shared memory\u548c\u5bc4\u5b58\u5668\u4e2d\u3002\\n\\n**\u540d\u8bcd\u5b9a\u4e49\uff1a** read SM write SM\u7528\u4e8e\u8bfb\u5199\u7684\u4e24\u5757shared memory\uff1bread REG write REG\u7528\u4e8e\u8bfb\u5199\u7684\u4e24\u5757\u5bc4\u5b58\u5668\\n\\n\u6bcf\u6b21\u5148\u628a\u4e0b\u4e00\u8f6e\u5927\u8fed\u4ee3\u8981\u7528\u5230\u7684\u6570\u636e\u5b58\u5230write SM\u4e2d\uff0c\u4e0b\u4e00\u8f6e\u5c0f\u8fed\u4ee3\u8981\u7528\u5230\u7684\u6570\u636e\u5b58\u5230write REG\u4e2d\u3002\u4e0a\u4e00\u8f6e\u5927\u8fed\u4ee3\u7684write SM\u5c31\u662f\u8fd9\u4e00\u8f6e\u8fed\u4ee3\u7684read SM\uff0c\u4e0a\u4e00\u8f6e\u5927\u8fed\u4ee3\u7684write REG\u5c31\u662f\u8fd9\u4e00\u8f6e\u8fed\u4ee3\u7684read REG\u3002\\n\\n\u7531\u4e8e\u4eceglobal memory\u4e2d\u53d6\u6570\u6240\u9700\u7684clock cycle\u975e\u5e38\u591a\uff0c\u6211\u4eec\u5728\u6570\u636e\u53d6\u56de\u7684\u540c\u65f6\u4e5f\u5bf9read SM\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u8ba1\u7b97\u3002\u5728\u6211\u4eec\u7b49\u5f85\u7684\u540c\u65f6\uff0c\u9700\u8981\u5f00\u542f8\u6b21\u5c0f\u8fed\u4ee3\u8fdb\u884c\u8ba1\u7b97\u3002\u800c\u5728\u5c0f\u8fed\u4ee3\u5185\u90e8\u4e5f\u5b58\u5728\u8bfb\u5199\u5206\u79bb\uff0c\u5728\u5bf9read REG\u8fdb\u884c\u8ba1\u7b97\u4e4b\u524d\uff0c\u9700\u8981\u5148\u6267\u884cwrite REG\u64cd\u4f5c\u3002\\n\\n### \u4ee3\u7801\u8be6\u89e3\\n\\n#### \u6a21\u7248\u53c2\u6570\\n\\nBLOCK_SIZE_M:128\\n\\nBLOCK_SIZE_N;8\\n\\nBLOCK_SIZE_N:128\\n\\nTHREAD_SIZE_Y:8\\n\\nTHREAD_SIZE_X:8\\n\\n\u5176\u4e2dtx ty\u662f\u6309\u7167cuda\u57fa\u672c\u8bbe\u7f6e\u5b9a\u4e49\u7684\uff0ctx\u4ee3\u8868\u6a2a\u5411\u5750\u6807\u27a1\ufe0f\uff0cty\u4ee3\u8868\u7eb5\u5411\u5750\u6807\u2b07\ufe0f\u3002\\n\\nTHREAD_X_PER_BLOCK=128/8=16\\n\\nTHREAD_Y_PER_BLOCK=128/8=16\\n\\nTHREAD_NUM_PER_BLOCK=16*16=256\\n\\ntid\u8868\u793a\u5f53\u524d\u7ebf\u7a0b\u5728\u8fd9256\u4e2a\u7ebf\u7a0b\u4e2d\u7684id\u53f7\\n\\ntid = ty * THREAD_X_PER_BLOCK + tx\\n\\n**As**\u4ee3\u8868\u4e3a\u4e86\u5b58\u50a8A\u77e9\u9635\u4e2d\u7684\u6570\u636e\u6240\u9700\u8981\u5f00\u542f\u7684shared memory\uff0c\u56e0\u4e3a\u6bcf\u6b21\u5c0f\u8fed\u4ee3\u4e2d\u53d6\u7684\u662fA\u7684\u4e00\u5c0f\u5217\uff0c\u4e3a\u4e86\u52a0\u5feb\u8bbf\u5b58\u901f\u5ea6\uff0c\u6211\u4eec\u5bf9A\u7684128\\\\*8\u7684\u5757\u8fdb\u884c\u8f6c\u7f6e\u540e\u5b58\u50a8\u5230 $b_m*b_k$\u7684\u5757\u4e2d\uff0c\u7531\u4e8e\u6709\u9884\u53d6\u8bbe\u7f6e\uff0c\u6211\u4eec\u5f00\u542f\u4e24\u500d\u7684\u5927\u5c0f\uff0c\u5171\u9700\u89812\\\\*BLOCK_SIZE_K\\\\*BLOCK_SIZE_M\u7684\u7a7a\u95f4\u3002\u5bf9\u4e8e**Bs**\uff0c\u7531\u4e8e\u6211\u4eec\u6bcf\u6b21\u5c0f\u8fed\u4ee3\u53d6B\u7684\u4e00\u5c0f\u884c\uff0c\u56e0\u6b64\u4e0d\u9700\u8981\u5bf9B\u8fdb\u884c\u8f6c\u7f6e\u3002\u5171\u9700\u89812*BLOCK_SIZE_K\\\\*BLOCK_SIZE_N\u7684\u7a7a\u95f4\u3002\\n\\n**accum**\u7528\u6765\u4e34\u65f6\u5b58\u50a8C\u7684\u8ba1\u7b97\u7ed3\u679c\u3002\\n\\n**frag_a**\u7528\u6765\u52a0\u8f7dAs\u4e2d\u7684rm\u4e2a\u6570\u636e\uff0c\u4e3a\u4e86\u6570\u636e\u9884\u53d6\u540c\u6837\u5f00\u542f\u4e24\u500d\u7a7a\u95f4\u3002**frag_b**\u540c\u7406\u3002\\n\\n**ldg_num_a**\u662f\u4eceglobal memory\u642c\u8fd0\u5230shared memory\u7684\u4e2d\u8f6c\u7ad9\uff0c\u4e00\u6b21\u5927\u8fed\u4ee3\u9700\u8981A\u4e2d\u7684128\\\\*8\u4e2a\u6570\u636e\uff0c\u4e00\u4e2ablock\u4e2d\u4e00\u5171\u6709256\u4e2a\u7ebf\u7a0b\uff0c\u4ee4\u4e00\u4e2a\u7ebf\u7a0b\u4e00\u6b21\u53d64\u4e2afloat\u6570\u636e\uff0c\u5219\u6bcf\u4e2a\u7ebf\u7a0b\u9700\u8981\u642c\u8fd0128\\\\*8/(256*4)=1\u6b21\u6570\u636e\uff08\u6b64\u5904\u60c5\u51b5\u6bd4\u8f83\u7279\u6b8a\uff0c\u5982\u679c\u4e00\u4e2ablock\u9700\u8981256\\\\*8\u4e2a\u6570\u636e\uff0c\u5219\u6bcf\u4e2a\u7ebf\u7a0b\u9700\u8981\u642c\u8fd02\u6b21\uff09\u3002**\u8fd9\u4e2a\u642c\u8fd0\u6b21\u6570\u5c31\u662fldg_num_a**\uff0c\u4e3a\u4e86\u5b9e\u73b0pipeline parallezation\uff0c\u6211\u4eec\u4e00\u5171\u51c6\u5907**ldg_a_reg**=ldg_num_a \\\\* 4\u4e2a\u5bc4\u5b58\u5668\u3002\\n\\n\u5728global memory -> shared memory\u9636\u6bb5\uff0c\u6211\u4eec\u9700\u8981\u642c\u8fd0128\\\\*8\u4e2a\u5143\u7d20\u5230shared memory\u4e2d\u3002**A_TILE_THREAD_PER_ROW** \u4ee3\u8868\u642c\u8fd0\u4e00\u884c\u6570\u636e\u9700\u8981\u51e0\u4e2a\u7ebf\u7a0b\uff0c\u6b64\u5904\u4e00\u884c8\u4e2a\u5143\u7d20\uff0c\u642c\u8fd0\u4e00\u884c\u9700\u89812\u4e2a\u7ebf\u7a0b\u3002**A_TILE_ROW_START**\u4ee3\u8868\u5728\u8fd9\u4e2a128\\\\*8\u7684\u6570\u636e\u5757\u4e2d\uff0c\u5f53\u524d\u7ebf\u7a0b\u9700\u8981\u642c\u8fd0\u7684\u6570\u636e\u6bb5\u7684\u7eb5\u5411\u5750\u6807(y)\uff0c**A_TILE_COL**\u4ee3\u8868\u5f53\u524d\u9700\u8981\u642c\u8fd0\u7684\u6570\u636e\u6bb5\u7684\u6a2a\u5411\u5750\u6807(x)\u3002\\n\\n\u7531\u4e8e128\\\\*8\u7684\u5757\u53ea\u7528\u642c\u8fd0\u4e00\u6b21\uff0c\u7528\u4e0d\u4e0a **A_TILE_COL**\uff0c\u6211\u4eec\u5728\u6b64\u4f7f\u7528256\\\\*8\u7684\u6570\u636e\u5757\u89e3\u91ca\u4e0a\u8ff0\u4e09\u4e2a\u53c2\u6570\u7684\u903b\u8f91\u3002 256\\\\* 8\u7684\u77e9\u9635\u4e58\u4ee58\\\\*256\u7684\u77e9\u9635\u5f97\u5230256\\\\*256\u7684\u77e9\u9635\uff0crm=rn=8\uff0c\u56e0\u6b64\u4e00\u5171\u670916*16\u4e2a\u7ebf\u7a0b\u3002\u5bf9\u4e8e(0,2)\u5750\u6807\u5757\uff0c\u4e5f\u5c31\u662f3\u53f7\u7ebf\u7a0b\u800c\u8a00\uff0c\u5b83\u9700\u8981\u642c\u8fd0\u7684\u662f:\\n\\n![image-20251101125400162](./image1.png)\\n\\n**A_TILE_ROW_STRIDE**\u4ee3\u8868\u5728\u591a\u6b21\u642c\u8fd0\u65f6\u7684\u884c\u8de8\u5ea6\u3002\\n\\n\u5177\u4f53\u8ba1\u7b97\u516c\u5f0f\u4e3a\uff1a\\n\\nA_TILE_ROW_START = tid / A_TILE_PER_ROW;\\n\\nA_TILE_COL = tid % A_TILE_PER_ROW;\\n\\nA_TILE_ROW_STRIDE = THREAD_NUM_PER_BLOCK / A_TILE_THREAD_PER_ROW;(\u4e5f\u5c31\u662f\u8bf4\u8ba1\u7b97\u51fa\u6765\u6240\u6709\u7ebf\u7a0b\u4e00\u8d77\u642c\u8fd0\u4e00\u6b21\u4e4b\u540e\u53ef\u4ee5\u642c\u8fd0\u591a\u5c11\u884c\uff0c\u8fd9\u4e2a\u5c31\u662f\u8de8\u5ea6)\\n\\n#### \u5177\u4f53\u4ee3\u7801\u903b\u8f91\\n\\n```c\\n#define OFFSET(row, col, ld) ((row) * (ld) + (col))\\n```\\n\\n\uff08ld for leading dimension\uff0c\u5728\u884c\u4e3b\u5e8f\u7684\u573a\u666f\u4e2d\u4ee3\u8868\u7684\u5c31\u662f\u6bcf\u884c\u6709\u51e0\u4e2a\u5143\u7d20\uff09\\n\\n##### Prefetch\\n\\n###### Part 1: global memory -> shared memory\\n\\n\u6b64\u5904\u9700\u8981\u6ce8\u610f\u6211\u4eec\u8981\u5bf9A\u7684\u5f85\u5199\u5757\u5148\u8fdb\u884c\u8f6c\u7f6e\u518d\u5b58\u5165\u3002\\n\\n\u6b64\u5904\u7701\u7565\u4e86\u4e2d\u95f4\u7684\u5bc4\u5b58\u5668\u4e2d\u8f6c\u73af\u8282global memory ( -> register ) -> shared memory\\n\\n```c\\n#pragma unroll\\nfor(int i = 0; i < BLOCK_SIZE_M; i += A_TILE_ROW_STRIDE){\\n\\tint ldg_index = i / A_TILE_ROW_STRIDE * 4;\\n  FETCH_FLOAT4(ldg_a_reg[ldg_index]) = FETCH_FLOAT4(A[OFFSET(BLOCK_SIZE_M * by + A_TILE_ROW_START + i, A_TILE_COL, K)]);\\n  As[0][A_TILE_COL][A_TILE_ROW_START + i] = ldg_a_reg[ldg_index];\\n\\tAs[0][A_TILE_COL + 1][A_TILE_ROW_START + i] = ldg_a_reg[ldg_index + 1];\\n  As[0][A_TILE_COL + 2][A_TILE_ROW_START + i] = ldg_a_reg[ldg_index + 2];\\n  As[0][A_TILE_COL + 3][A_TILE_ROW_START + i] = ldg_a_reg[ldg_index + 3];\\n}\\n#pragma unroll\\nfor(int i = 0; i < BLOCK_SIZE_K; i += B_TILE_ROW_STRIDE){\\n  FETCH_FLOAT4(Bs[0][B_TILE_ROW_START + i][B_TILE_COL]) = FETCH_FLOAT4(B[OFFSET(B_TILE_ROW_START + i, B_TILE_COL + BLOCK_SIZE_N * bx, N)]);\\n}\\n_syncthreads();\\n```\\n\\n```c\\nFETCH_FLOAT4(ldg_a_reg[ldg_index]) = FETCH_FLOAT4(A[OFFSET(\\n            BLOCK_SIZE_M * by + A_TILE_ROW_START + i, // row\\n            A_TILE_COL, // col\\n            K )]);\\n```\\n\\n\u6b64\u5904\u7684BLOCK_SIZE_M \\\\* by\u4e2d\u7684by\u662f\u5168\u5c40\u7684\u5927\u5750\u6807\u5bf9\u5e94\u7684\u6a2a\u5750\u6807\uff0cA_TILE_ROW_START\u662f\u5f53\u524d\u5757\u5185\u5c0f\u7684\u884c\u5750\u6807\uff0ci\u662fSTRIDE\u3002\\n\\n==???== \u4f46\u6709\u4e2a\u7591\u95ee\u70b9\uff1a\u4e3a\u4ec0\u4e48col\u76f4\u63a5\u5c31\u662fA_TILE_COL\uff0c\u4e0d\u5e94\u8be5\u540crow\u4e00\u6837\uff0c+ BLOCK_SIZE_K \\\\* bx\u5417\uff1f\\n\\n==ans==:\u5728\u5b8c\u6574\u7684\u4ee3\u7801\u4e2d\u5e94\u8be5\u8fd8\u6709\u4e00\u4e2a\u5916\u5c42\u5faa\u73af\uff0c\u7528\u4e8e\u63a7\u5236k\uff0c\u6b64\u5904\u53ea\u662f\u7b2c\u4e00\u6b21\u8fed\u4ee3\u7684\u9884\u53d6\uff0ck=0\\n\\n==\u9700\u8981\u627e\u5b8c\u6574\u4ee3\u7801\u68c0\u9a8c==\\n\\n\\n\\n\\n\\n```c\\n#pragma unroll\\n```\\n\\nCUDA\u7f16\u8bd1\u5668\u6307\u4ee4\uff0c\u7528\u4e8e\u5faa\u73af\u5c55\u5f00\u4f18\u5316\u3002\\n\\n\u4e3e\u4f8b\uff1a\u5c55\u5f00\u524d\uff1a\\n\\n```c\\nfor (int i = 0; i < 4; i++) {\\n    result += array[i];\\n}\\n```\\n\\n\u5c55\u5f00\u540e\uff1a\\n\\n```c\\nresult += array[0];\\nresult += array[1]; \\nresult += array[2];\\nresult += array[3];\\n```\\n\\n\u4f18\u52bf\uff1a\u6d88\u9664\u5faa\u73af\u5f00\u9500\uff0c\u63d0\u9ad8\u6307\u4ee4\u5373\u5e76\u884c\uff0c\u4fbf\u4e8e\u5bc4\u5b58\u5668\u5206\u914d\u3002\\n\\nlimit\uff1a\u4ec5\u9002\u7528\u4e8e\u5faa\u73af\u6b21\u6570\u5c11\u4e14\u56fa\u5b9a\u7684\u60c5\u51b5\\n\\n==try==:\u5728\u6d4b\u8bd5\u4e2d\u53ef\u4ee5\u5c1d\u8bd5\u6ce8\u91ca\u6389\u8fd9\u884c\u8bed\u53e5\uff0c\u6bd4\u8f83\u6027\u80fd\\n\\n\\n\\n```c\\n_syncthreads();\\n```\\n\\nCUDA\u540c\u6b65\u5c4f\u969c\u51fd\u6570\uff0c\u786e\u4fdd\u5728\u540c\u4e00\u4e2a\u7ebf\u7a0b\u5757\u4e2d\u7684\u6240\u6709\u7ebf\u7a0b\u90fd\u6267\u884c\u5230_syncthreads()\u8fd9\u4e2a\u4f4d\u7f6e\uff0c\u4fdd\u8bc1\u5728_syncthreads()\u4e4b\u524d\u7684\u6240\u6709\u5185\u5b58\u64cd\u4f5c\u5bf9\u5176\u4ed6\u7ebf\u7a0b\u53ef\u89c1\\n\\n\\n\\n###### Part 2\uff1ashared memory -> register\\n\\n```c\\n#pragma unroll\\nfor(int thread_y = 0; thread_y < THREAD_SIZE_Y; thread_y += 4){\\n\\tFETCH_FLOAT4(frag_a[0][thread_y]) = FETCH_FLOAT4(As[0][0][THREAD_SIZE_Y * ty + thread_y]);\\n}\\n#pragma unroll\\nfor(int thread_x = 0; thread_x < THREAD_SIZE_X; thread_x += 4){\\n\\tFETCH_FLOAT4(frag_b[0][thread_x]) = FETCH_FLOAT4(Bs[0][0][THREAD_SIZE_X * tx + thread_x]);\\n}\\n```\\n\\n\u540c\u6837\u7684\uff0c\u8bb0\u4f4ftx, ty\u90fd\u662f\u5168\u5c40\u5927\u5750\u6807\uff0c\u800c\u4e14\u7531\u4e8e\u6b64\u5904\u662f\u4eceshared  memory -> register\uff0c\u56e0\u6b64\u5bf9\u4e8eA\u77e9\u9635\u4e0d\u9700\u8981\u8003\u8651\u6a2a\u5411\u7684 block \u504f\u79fb\uff0c\u5bf9\u4e8eB\u77e9\u9635\u4e0d\u9700\u8981\u8003\u8651\u7eb5\u5411\u7684 block \u504f\u79fb\u3002\u6211\u4eec\u6bcf\u6b21\u642c\u7684\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a4\u7684\u5c0f\u6761\uff0c\u5b8c\u6210\u8fd9\u4e2afor\u5faa\u73af\u624d\u662f\u642c\u4e86\u4e00\u4e2a\u957f\u5ea6\u4e3a8\u7684\u5355\u5143\u6761\u3002\\n\\n![image-20251101144036676](./image2.png)\\n\\n##### \u5927\u8fed\u4ee3\u903b\u8f91\\n\\n**write_stage_idx**\u5982\u679c=1\uff0c\u5219\u5bf9As[1]\u7a7a\u95f4\u8fdb\u884c\u5199\u64cd\u4f5c\uff0c\u5bf9As[0]\u7a7a\u95f4\u8fdb\u884c\u8bfb\u64cd\u4f5c\u3002\\n\\n**tile_idx**\u8868\u793a\u5728\u5927\u8fed\u4ee3\u65f6\uff0c\u5728A\u77e9\u9635\u7684\u5217\u53f7\u3002\\n\\n![image-20251101144614251](./image3.png)\\n\\n\u6bcf\u6b21\u5927\u8fed\u4ee3\u8981\u8bfbBLOCK_SIZE_K\u5217\uff0c\u76f4\u5230\u5b8c\u6210\u5927\u8fed\u4ee3\uff0c\u4e5f\u5c31\u662ftile_idx=K\u4e3a\u6b62\u3002\\n\\n**load_stage_idx**\u548cwrite_stage_idx\u5bf9\u5e94\uff0c\u4e8c\u8005\u4fdd\u6301\u4e8c\u8fdb\u5236\u4f4d\u76f8\u53cd\u5373\u53ef\\n\\n```c\\nint wirte_stage_idx = 1;\\nint tile_idx = 0;\\ndo{\\n\\ttile_idx += BLOCK_SIZE_K;\\n\\tint load_stage_idx = write_stage_idx ^ 1;\\n\\tif(tile_idx < K){\\n\\t\\twrite_stage_idx ^= 1;\\n\\t}\\n}while(tile_idx < K);\\n```\\n\\n**\u5927\u8fed\u4ee3\u8be6\u89e3**\uff1a\\n\\n```c\\ntile_idx += BLOCK_SIZE_K;\\nif(tile_idx < K){\\n  #pragma unroll\\n  for(int i = 0; i < BLOCK_SIZE_M; i += A_TILE_ROW_STRIDE){\\n    int ldg_index = i / A_TILE_ROW_STRIDE * 4;\\n    FETCH_FLOAT4(ldg_a_reg[ldg_index]) = FETCH_FLOAT4(A[OFFSET(BLOCK_SIZE_M * by + A_TILE_ROW_START + i, A_TILE_COL + tile_idx, K)]);\\n  }\\n  #pragma unroll\\n  for(int i = 0; i < BLOCK_SIZE_K; i += B_TILE_ROW_STRIDE){\\n    int ldg_index = i / B_TILE_ROW_STRIDE * 4;\\n    FETCH_FLOAT4(ldg_b_reg[ldg_index]) = FETCH_FLOAT4(B[OFFSET(tile_idx + B_TILE_ROW_START + i, B_TILE_COL + BLOCK_SIZE_N * bx, N)]);\\n  }\\n}\\n```\\n\\n==???==\u6ca1\u592a\u660e\u767d\u7d22\u5f15\u8ba1\u7b97\u903b\u8f91\\n\\n\\n\\n\u968f\u540e\u8fdb\u5165\u5c0f\u8fed\u4ee3\u8ba1\u7b97\u903b\u8f91\u4e2d\uff0cload_stage_idx\u8868\u793a\u9700\u8981\u4eceAs\u7684\u54ea\u4e2a\u7a7a\u95f4\u8fdb\u884c\u8bfb\u6570\u3002\u7136\u540e\u662f **BLOCK_SIZE_K-1**\u6b21\u5c0f\u8fed\u4ee3\u3002\u672c\u4f8b\u4e2d\u5c31\u662f7\u6b21\u5c0f\u8fed\u4ee3\u3002\u5728\u5bf9 shared memory\u8fdb\u884c\u8bbf\u5b58\u7684\u540c\u65f6\u8ba1\u7b97THREAD_SIZE_X \\\\* THREAD_SIZE_Y\u4e2aC\u77e9\u9635\u5143\u7d20\u7684\u7ed3\u679c\u3002\\n\\n```c\\nint load_stage_idx = write_stage_idx ^ 1;\\n#pragmatism unroll\\nfor(int j = 0; j < BLOCK_SIZE_K - 1; ++j){\\n  #pragma unroll\\n  for(int thread_y = 0; thread_y < THREAD_SIZE_Y; thread_y += 4){\\n    FETCH_FLOAT4(frag_a[(j + 1) % 2][thread_y]) = FETCH_FLOAT_4(As[load_stage_idx][j + 1][THREAD_SIZE_Y * ty + thread_y])\\n  }\\n  #pragma unroll\\n  for(int thread_x = 0; thread_x < THREAD_SIZE_X; thread_x += 4){\\n    FETCH_FLOAT4(frag_b[(j + 1) % 2][thread_x]) = FETCH_FLOAT4(Bs[load_stage_idx][j + 1][THREAD_SIZE_X * tx + thread_x]);\\n  }\\n  #pragma unroll\\n  for(int thread_x = 0; thread_y < THREAD_SIZE_Y; ++thread_y){\\n    for(int thread_x = 0; thread_x < THREAD_SIZE_Y; ++thread_y){\\n      accumulation[thread_y][thread_x] += frag_a[j % 2][thread_y] * frag_b[j % 2][thread_x];\\n    }\\n  }\\n}\\n```\\n\\n![image-20251109092739978](./image4.png)"}]}}')}}]);