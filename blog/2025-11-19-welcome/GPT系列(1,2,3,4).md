# GPT系列(1,2,3,4)

## GPT1--Improving Language Understanding by Generative Pre-Training

#### **动机**

NLP每个任务都需要大量标注数据，模型不能复用

CV领域得益于imagenet数据集，预训练模型，下游任务可以微调。

OPEN AI要做NLP领域的预训练模型

#### **NLP领域预训练模型难点**

1. 没有像ImageNet那样大量的标注数据
   **Solution：**用语言模型自回归的方式来训练模型
2. 预训练模型架构轻微修改，可以应用到下游任务。模型架构怎么设计？
   **Solution**：选择Transformer（能更好记住训练数据中的模式，也能更好迁移到下游任务），**且只使用Decoder**
   **架构参数**：
   12层Decoder Block
   hidden dim：768
   12 heads
   1.17亿参数
   可学习的位置Embedding
   GELU激活函数

<img src="/blog/2025-11-19-welcome/gpt_image/1.png" style="zoom:50%;" />

#### 预训练

**语言模型似然函数**：$L_1(u)=\underset{i}{\sum}logP(u_i|u_{i-k},...,u_{i-1};𝚯)$

如何得到下一个词的概率：

$h_0=UW_e+W_p$

$h_w=decoder_block(h_{l-1}∀l\in[1,n])$

$P(u)=softmax(h_nW_e^T)$

**注意：**

在GPT1中，$h_0$和$P(u)$中的$W_e$是共享的，$W_e$将字典长度转换为隐藏层的维度，$W_e^T$将隐藏层的维度转换为字典长度。因为此时的模型较小，$W_e$就占参数很大一部分。随后随着模型越来越大，有些模型选择独立训练分类头。

#### 微调

既关注下游任务，又关注自回归训练本身，通过给两个任务的Loss分配不一样的权重，协同训练模型

![image-20251211000807486](blog/2025-11-19-welcome/gpt_image/2.png)

#### 训练数据

BooksCorpus Dataset（7000本未发表的书）。（800M words）

上下文长度512

BatchSize：32

## GPT2--Language Models are Unsupervised Multitask Learners

#### **动机**

GPT1和Bert将预训练模型引入了NLP，但下游任务仍然需要收集一部分数据，进行微调。

GPT2希望通过预训练模型可以解决所有下游任务

#### 怎么解决下游任务

在GPT1，在微调时引入起始符，分割符，结束符来让模型进行下游任务。利用input直接产生output
$$
p(output|input)
$$
GPT2不进行微调，只能用预训练时见过的token，所以自然的引入了Prompt
$$
p(output|input,task)
$$
**例如**

情感分类任务：

GPT1:<start>今天这家餐厅的服务真的很棒！<extract>

GPT2:判断下面这个句子的情绪是正面的还是负面的：今天这家餐厅的服务真的很棒！

#### 模型修改

1. 将Layer Norm移动到每个block的输入位置，最后一个子层的自注意机制后加上了一个Layer Norm。这样让训练更稳定。
2. 残差层的初始化参数，随着层数增加而减小，除以根号下N。N是残差层的层数。这样便于梯度向前传递
3. 扩大了词典到50257。
4. 模型参数达到15.42亿。

#### 训练修改

1. 上下文长度从512扩展到1024
2. BatchSize扩展到512
3. 使用Reddit优质网页，800万文本，40GB文字

**随着模型参数增大，性能一直有上升趋势**

## GPT3--Language Models are Few-Shot Learners

#### 动机

人类在做一个语言任务时，只要给几个例子就可以。但是传统Bert，GPT1，还需要成千上万的下游任务的例子。

能否通过在Prompt里给模型提供几个例子，提升模型性能。（Few-shot）

延续GPT2的思想，继续扩大模型体量

![image-20251211003205546](blog/2025-11-19-welcome/gpt_image/3.png)

#### 模型修改

和GPT2一样，引入了稀疏注意力机制。

#### 训练数据

![image-20251211003517802](blog/2025-11-19-welcome/gpt_image/4.png)

上下文长度2048

Batch Size320万

#### 训练结果

![image-20251211003608415](blog/2025-11-19-welcome/gpt_image/5.png)

![image-20251211003641636](blog/2025-11-19-welcome/gpt_image/6.png)

## GPT4

#### 一点有用的见解

模型的能力是在预训练的时候获得的，RLHF只是和人类意识对齐，并不能提高模型表现。

如果RLHF做得不好，还可能破坏模型原有的能力

#### 利用小模型预知大模型能力

![image-20251211004059533](blog/2025-11-19-welcome/gpt_image/7.png)